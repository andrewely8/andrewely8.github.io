{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unsloth trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Nde7BypNxig0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsQfdLxWdEMm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from google.colab import userdata, files\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "data_files = {\n",
        "    \"train\": \"train_2000.json\",\n",
        "    \"validation\": \"validation.json\",\n",
        "    \"test\": \"test.json\",\n",
        "}\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "train_ds = dataset[\"train\"]\n",
        "val_ds   = dataset[\"validation\"]\n",
        "test_ds  = dataset[\"test\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None          # let Unsloth decide\n",
        "dtype = None\n",
        "\n",
        "model_name = \"unsloth/Qwen2.5-Math-1.5B\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "tokenizer = get_chat_template(tokenizer,chat_template = \"qwen25\",)\n"
      ],
      "metadata": {
        "id": "LBWlQZrCduUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(example):\n",
        "    q = example[\"question\"]\n",
        "    a = example[\"answer\"]\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"please reason step by step and put final answer within \\\\boxed{}.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": q,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": a,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=False,)\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_sft = train_ds.map(format_prompt)\n",
        "val_sft   = val_ds.map(format_prompt)\n"
      ],
      "metadata": {
        "id": "JT3kUdkBFH8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_model(prompt):\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True,)\n",
        "\n",
        "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs,max_new_tokens=512)\n",
        "\n",
        "  generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "\n",
        "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "vJ0SaQ97e9DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractAnswer(response):\n",
        "    matches = re.findall(r\"boxed\\s*\\{([^}]*)\\}\", response)\n",
        "    for content in reversed(matches):\n",
        "        content = content.strip()\n",
        "        if content:\n",
        "            return content\n",
        "    return \"Invalid Response Format\"\n",
        "\n",
        "def calculateAccuracy(predicted,labeled):\n",
        "  correct = 0\n",
        "  for i in range(len(predicted)):\n",
        "    if predicted[i] == labeled[i]:\n",
        "      correct += 1\n",
        "  return (correct/len(predicted))\n",
        "\n",
        "def model_assesment(hf_dataset, savefile_name):\n",
        "    n = len(hf_dataset)\n",
        "    responses = []\n",
        "    labeled_answers = []\n",
        "    predicted_answers = []\n",
        "\n",
        "    for i, example in enumerate(hf_dataset):\n",
        "        question = example[\"question\"]\n",
        "        label = example[\"answer\"]\n",
        "\n",
        "        # Save ground-truth\n",
        "        actualAnswer = extractAnswer(label)\n",
        "        labeled_answers.append(actualAnswer)\n",
        "\n",
        "        # Run model\n",
        "        response = prompt_model(question)\n",
        "        responses.append(response)\n",
        "\n",
        "        # Extract boxed answer\n",
        "        predicted = extractAnswer(response)\n",
        "        predicted_answers.append(predicted)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Predicted answer: \", predicted)\n",
        "        print(\"actual answer: \", actualAnswer)\n",
        "        print(f\"progress: {i+1}/{n}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    accuracy = calculateAccuracy(predicted_answers, labeled_answers)\n",
        "\n",
        "    results = {\n",
        "        \"responses\": responses,\n",
        "        \"predicted\": predicted_answers,\n",
        "        \"labeled\": labeled_answers,\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "\n",
        "    with open(f\"{savefile_name}.json\", \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(f\"Saved results to {savefile_name}.json\")\n",
        "    print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "EfKf-148giUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_assesment(test_ds,\"test_baseline\")"
      ],
      "metadata": {
        "id": "02VOPHNdknb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_assesment(val_ds,\"validation_baseline\")"
      ],
      "metadata": {
        "id": "mj49jm06yZYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA to the base model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0.1,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LqLWPbVvwUVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v08de3wAXdu6"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,  # effective batch = 8\n",
        "    warmup_steps = 10,\n",
        "    num_train_epochs = 2,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 25,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"outputs\",\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit = 2,\n",
        "    dataloader_pin_memory = False,\n",
        "    report_to = \"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_sft,\n",
        "    eval_dataset = val_sft,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "lnNgd1DjxD_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validation\n",
        "model_assesment(val_ds,\"validation_finetuned\")"
      ],
      "metadata": {
        "id": "uLX7sv3KyCsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "model_assesment(test_ds,\"test_finetuned\")"
      ],
      "metadata": {
        "id": "P4zXv-6yyLUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "metadata": {
        "id": "wPGM3v0SxK9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "if gguf_files:\n",
        "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "    print(f\"Downloading: {gguf_file}\")\n",
        "    files.download(gguf_file)"
      ],
      "metadata": {
        "id": "cI2NNKK3xOa5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
